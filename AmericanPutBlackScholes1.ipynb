{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":378,"status":"ok","timestamp":1699280271174,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"2OfqWqfxZW9K"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math\n","import pandas as pd\n","import tensorflow_probability as tfp\n","from scipy.interpolate import griddata\n","from scipy import stats\n","import timeit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16921,"status":"ok","timestamp":1699280288846,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"tJ7xr08niun7","outputId":"a744873e-f642-4db5-b8d3-7e222193d501"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":540,"status":"ok","timestamp":1699280289383,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"kBD7qyOc3LxW"},"outputs":[],"source":["class Sampler:\n","    def __init__(self, dim, coords, func, name = None):\n","        self.dim = dim\n","        self.coords = coords\n","        self.func = func\n","        self.name = name\n","    def sample(self, N):\n","        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n","        y = self.func(x)\n","        return x, y\n","\n","class Black_Scholes:\n","\n","    # Disable eager execution\n","    tf.compat.v1.disable_eager_execution()\n","\n","    def __init__(self, layers, operator, ic, bc, res, r, sigma, delta, K, model, stiff_ratio):\n","\n","        self.operator = operator\n","        self.ic = ic\n","        self.bc = bc\n","        self.res = res\n","\n","        self.model = model\n","        self.stiff_ratio = stiff_ratio\n","\n","        # Constant\n","        self.sigma = tf.constant(sigma, dtype=tf.float32)\n","        self.r = tf.constant(r, dtype = tf.float32)\n","        self.delta = tf.constant(delta, dtype = tf.float32)\n","        self.K = tf.constant(K, dtype = tf.float32)\n","\n","        # Decay rate = 0.9\n","        self.rate = 0.9\n","        self.adaptive_constant_ic_val = np.array(1.0) # konstanta ic\n","        self.adaptive_constant_bc_val = np.array(1.0) # konstanta bc\n","\n","        # Initial weight + bias\n","        self.layers = layers\n","        self.weight, self.bias = self.initialize_NN(layers)\n","\n","        if model in ['M3', 'M4']:\n","            self.encoder_weight_1 = self.xavier_init([2, layers[1]])\n","            self.encoder_bias_1 = self.xavier_init([1,layers[1]])\n","\n","            self.encoder_weight_2 = self.xavier_init([2, layers[1]])\n","            self.encoder_bias_2 = self.xavier_init([1, layers[1]])\n","\n","        self.sess = tf.compat.v1.Session(config = tf.compat.v1.ConfigProto(log_device_placement = True))\n","\n","        # Define Placeholders and Computational Graph\n","        self.t_v_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_v_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.t_ic_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_ic_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.v_ic_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.t_bc1_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_bc1_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.v_bc1_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.t_r_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_r_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.res_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.adaptive_constant_ic_tf = tf.compat.v1.placeholder(tf.float32, shape=self.adaptive_constant_ic_val.shape)\n","        self.adaptive_constant_bc_tf = tf.compat.v1.placeholder(tf.float32, shape=self.adaptive_constant_bc_val.shape)\n","\n","        # Evaluate Predictions\n","        self.v_ic_pred = self.net_u(self.t_ic_tf, self.s_ic_tf)\n","        self.v_bc1_pred = self.net_u(self.t_bc1_tf, self.s_bc1_tf)\n","\n","        self.v_pred = self.net_u(self.t_v_tf, self.s_v_tf)\n","        self.res_pred = self.net_r(self.t_r_tf, self.s_r_tf)\n","\n","        # Initial and Boundary Loss\n","        self.loss_ic = tf.reduce_mean(tf.square(self.v_ic_tf - self.v_ic_pred))\n","        self.loss_bc1 = tf.reduce_mean(tf.square(self.v_bc1_pred - self.v_bc1_tf))\n","\n","        self.loss_bcs = self.adaptive_constant_bc_tf * (self.loss_bc1)\n","        self.loss_ics = self.adaptive_constant_ic_tf * (self.loss_ic)\n","        self.loss_v = self.loss_ics + self.loss_bcs\n","\n","        self.loss_res = tf.reduce_mean(tf.square(self.res_pred))\n","        # - self.res_tf\n","        # Total Loss\n","        self.loss = self.loss_res + self.loss_v\n","\n","        # Define optimizer with learning rate schedule\n","        self.global_step = tf.Variable(0, trainable=False)\n","        initial_learning_rate = 1e-3\n","        self.learning_rate = tf.compat.v1.train.exponential_decay(initial_learning_rate, self.global_step,\n","                                                        1000, 0.9, staircase=False)\n","\n","        # Passing global_step to minimize() will increment it at each step\n","        self.train_op = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step = self.global_step)\n","\n","        # Logger\n","        self.loss_ic_log = []\n","        self.loss_bc_log = []\n","        self.loss_r_log = []\n","        self.saver = tf.compat.v1.train.Saver()\n","\n","        # Generate dictionary for gradients storage\n","        self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n","        self.dict_gradients_ic_layers = self.generate_grad_dict(self.layers)\n","        self.dict_gradients_bc_layers = self.generate_grad_dict(self.layers)\n","\n","        # Gradients Storage\n","        self.grad_res = []\n","        self.grad_ic = []\n","        self.grad_bc = []\n","\n","        for i in range(len(self.layers) - 1):\n","            self.grad_res.append(tf.gradients(self.loss_res, self.weight[i])[0])\n","            self.grad_bc.append(tf.gradients(self.loss_bcs, self.weight[i])[0])\n","            self.grad_ic.append(tf.gradients(self.loss_ics, self.weight[i])[0])\n","\n","        # Store Adaptive Constant\n","        self.adaptive_constant_ic_log = []\n","        self.adaptive_constant_bc_log = []\n","\n","        # Compute the adaptive constant\n","        self.adaptive_constant_ic_list = []\n","        self.adaptive_constant_bc_list = []\n","\n","        self.max_grad_res_list = []\n","        self.mean_grad_bc_list = []\n","        self.mean_grad_ic_list = []\n","\n","        for i in range(len(self.layers) - 1):\n","            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i])))\n","            self.mean_grad_bc_list.append(tf.reduce_mean(tf.abs(self.grad_bc[i])))\n","            self.mean_grad_ic_list.append(tf.reduce_mean(tf.abs(self.grad_ic[i])))\n","\n","        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n","        self.mean_grad_bc = tf.reduce_mean(tf.stack(self.mean_grad_bc_list))\n","        self.mean_grad_ic = tf.reduce_mean(tf.stack(self.mean_grad_ic_list))\n","\n","        self.adaptive_constant_bc = self.max_grad_res / self.mean_grad_bc\n","        self.adaptive_constant_ic = self.max_grad_res / self.mean_grad_ic\n","\n","        # Stiff Ratio\n","        if self.stiff_ratio:\n","            self.Hessian, self.Hessian_ic, self.Hessian_bc, self.Hessian_res = self.get_H_op()\n","            self.eigenvalue, _ = tf.linalg.eigh(self.Hessian)\n","            self.eigenvalue_ic, _ = tf.linalg.eigh(self.Hessian_ic)\n","            self.eigenvalue_bc, _ = tf.linalg.eigh(self.Hessian_bc)\n","            self.eigenvalue_res, _ = tf.linalg.eigh(self.Hessian_res)\n","\n","            self.eigenvalue_log = []\n","            self.eigenvalue_ic_log = []\n","            self.eigenvalue_bc_log = []\n","            self.eigenvalue_res_log = []\n","\n","        # Initialize tensorflow variables\n","        init = tf.compat.v1.global_variables_initializer()\n","        self.sess.run(init)\n","\n","    # Create dictionary to store gradients\n","    def generate_grad_dict(self, layers):\n","        num = len(layers) - 1\n","        grad_dict = {}\n","        for i in range(num):\n","            grad_dict['layer_{}'.format(i + 1)] = []\n","        return grad_dict\n","\n","    # Save gradients\n","    def save_gradients(self, tf_dict):\n","        num_layers = len(self.layers)\n","        for i in range(num_layers - 1):\n","            grad_ic_value, grad_bc_value, grad_res_value = self.sess.run([self.grad_ic[i], self.grad_bc[i], self.grad_res[i]], feed_dict = tf_dict)\n","\n","            self.dict_gradients_ic_layers['layer_' + str(i + 1)].append(grad_ic_value.flatten())\n","            self.dict_gradients_bc_layers['layer_' + str(i + 1)].append(grad_bc_value.flatten())\n","            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n","        return None\n","\n","    # Compute Hessian\n","    def flatten(self, vectors):\n","        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis = 0)\n","\n","    def get_Hv(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_Hv_ic(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss_ics, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_Hv_bc(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_Hv_res(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss_res, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_H_op(self):\n","        self.P = self.flatten(self.weight).get_shape().as_list()[0]\n","        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype = 'float32')\n","        H_ic = tf.map_fn(self.get_Hv_ic, tf.eye(self.P, self.P), dtype = 'float32')\n","        H_bc = tf.map_fn(self.get_Hv_bc, tf.eye(self.P, self.P), dtype = 'float32')\n","        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P), dtype = 'float32')\n","        return H, H_ic, H_bc, H_res\n","\n","    # Xavier initialization\n","    def xavier_init(self, size):\n","        in_dim = size[0]\n","        out_dim = size[1]\n","        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n","        return tf.Variable(tf.compat.v1.random_normal([in_dim, out_dim], dtype = tf.float32) * xavier_stddev, dtype=tf.float32)\n","\n","    # Initialize network weights and biases using Xavier initialization\n","    def initialize_NN(self, layers):\n","        weight = []\n","        bias = []\n","        num_layers = len(layers)\n","        for l in range(0, num_layers - 1):\n","            W = self.xavier_init(size = [layers[l], layers[l+1]])\n","            b = tf.Variable(tf.zeros([1, layers[l+1]], dtype = tf.float32), dtype = tf.float32)\n","            weight.append(W)\n","            bias.append(b)\n","        return weight, bias\n","\n","    # Evaluate the forward pass\n","    def forward_pass(self, H):\n","        if self.model in ['M1', 'M2']:\n","            num_layers = len(self.layers)\n","            for l in range(0, num_layers - 2):\n","                W = self.weight[l]\n","                b = self.bias[l]\n","                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n","            W = self.weight[-1]\n","            b = self.bias[-1]\n","            H = tf.add(tf.matmul(H, W), b)\n","            return H\n","\n","        if self.model in ['M3', 'M4']:\n","            num_layers = len(self.layers)\n","            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weight_1), self.encoder_bias_1))\n","            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weight_2), self.encoder_bias_2))\n","\n","            for l in range(0, num_layers - 2):\n","                W = self.weight[l]\n","                b = self.bias[l]\n","                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n","                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n","\n","            W = self.weight[-1]\n","            b = self.bias[-1]\n","            H = tf.add(tf.matmul(H, W), b)\n","            return H\n","\n","    # Forward pass for u\n","    # This code is for initial and boundary condition\n","    def net_u(self, t, s):\n","        v = self.forward_pass(tf.concat([t, s], 1))\n","        return v\n","\n","    # Forward pass for residual\n","    def net_r(self, t, s):\n","        v = self.net_u(t, s)\n","        residual = self.operator(v, t, s, self.r, self.sigma, self.delta, self.K)\n","        return residual\n","\n","    def fetch_minibatch(self, sampler, N):\n","        X, Y = sampler.sample(N)\n","        return X, Y\n","\n","    # Trains the model by minimizing the MSE loss\n","    def train(self, nIter = 10000):\n","        start_time = timeit.default_timer()\n","\n","        # Fetch boundary mini-batches\n","        s_ic_batch, v_ic_batch = self.fetch_minibatch(self.ic, 250)\n","        s_bc1_batch, v_bc1_batch = self.fetch_minibatch(self.bc, 250)\n","\n","        # Fetch residual mini-batch\n","        s_res_batch, f_res_batch = self.fetch_minibatch(self.res, 62500)\n","\n","        # Define a dictionary for associating placeholder with data\n","        tf_dict = {self.t_ic_tf: s_ic_batch[:, 0:1], self.s_ic_tf: s_ic_batch[:, 1:2],\n","                    self.v_ic_tf: v_ic_batch,\n","                    self.t_bc1_tf: s_bc1_batch[:, 0:1], self.s_bc1_tf: s_bc1_batch[:, 1:2],\n","                    self.v_bc1_tf: v_bc1_batch,\n","                    self.t_r_tf: s_res_batch[:, 0:1], self.s_r_tf: s_res_batch[:, 1:2],\n","                    self.res_tf: f_res_batch,\n","                    self.adaptive_constant_ic_tf: self.adaptive_constant_ic_val,\n","                    self.adaptive_constant_bc_tf: self.adaptive_constant_ic_val}\n","\n","        for it in range(nIter):\n","\n","            # Run the tensorflow session to minimize losss\n","            self.sess.run(self.train_op, tf_dict)\n","\n","            # Print\n","            if it % 10 == 0:\n","                elapsed = timeit.default_timer() - start_time\n","                loss_value = self.sess.run(self.loss, tf_dict)\n","                loss_ic_value, loss_bc_value, loss_r_value = self.sess.run([self.loss_ics, self.loss_bcs, self.loss_res], tf_dict)\n","\n","                if self.model in ['M2', 'M4']:\n","                    # Compute the adaptive constant\n","                    adaptive_constant_ic_val, adaptive_constant_bc_val = self.sess.run([self.adaptive_constant_ic, self.adaptive_constant_bc], tf_dict)\n","\n","                    self.adaptive_constant_ic_val = adaptive_constant_ic_val * (1.0 - self.rate) + self.rate * self.adaptive_constant_ic_val\n","                    self.adaptive_constant_bc_val = adaptive_constant_bc_val * (1.0 - self.rate) + self.rate * self.adaptive_constant_bc_val\n","\n","                # Store loss and adaptive weights\n","                self.loss_ic_log.append(loss_ic_value)\n","                self.loss_bc_log.append(loss_bc_value)\n","                self.loss_r_log.append(loss_r_value)\n","\n","                self.adaptive_constant_ic_log.append(self.adaptive_constant_ic_val)\n","                self.adaptive_constant_bc_log.append(self.adaptive_constant_bc_val)\n","\n","                print('It: %d, Loss: %.3e, Loss_u0: %.3e, Loss_ub: %.3e, Loss_r: %.3e, Time: %.2f' %\n","                      (it, loss_value, loss_ic_value, loss_bc_value, loss_r_value, elapsed))\n","                print(\"constant_ic_val: {:.3f}, constant_bc_val: {:.3f}\".format(\n","                    self.adaptive_constant_ic_val,\n","                    self.adaptive_constant_bc_val))\n","                start_time = timeit.default_timer()\n","\n","            # Compute the eigenvalues of the Hessian of losses\n","            if self.stiff_ratio:\n","                if it % 1000 == 0:\n","                    print(\"Eigenvalues information stored...\")\n","                    eigenvalue, eigenvalue_ic, eigenvalue_bc, eigenvalue_res = self.sess.run([self.eigenvalue, self.eigenvalue_ic, self.eigenvalue_bc, self.eigenvalue_res], tf_dict)\n","\n","                    self.eigenvalue_log.append(eigenvalue)\n","                    self.eigenvalue_ic_log.append(eigenvalue_ic)\n","                    self.eigenvalue_bc_log.append(eigenvalue_bc)\n","                    self.eigenvalue_res_log.append(eigenvalue_res)\n","\n","            if it % 10000 == 0:\n","                self.save_gradients(tf_dict)\n","                print(\"Gradients information stored...\")\n","\n","    # Evaluates predictions at test points\n","    def predict_v(self, s_star):\n","            tf_dict = {self.t_v_tf: s_star[:, 0:1], self.s_v_tf: s_star[:, 1:2]}\n","            v_star = self.sess.run(self.v_pred, tf_dict)\n","            return v_star\n","\n","    def predict_r(self, s_star):\n","        tf_dict = {self.t_r_tf: s_star[:, 0:1], self.s_r_tf: s_star[:, 1:2]}\n","        r_star = self.sess.run(self.res_pred, tf_dict)\n","        return r_star"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1699280289384,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"Q0muTmxyZitX"},"outputs":[],"source":["K = 10\n","r = 0.05\n","sigma = 0.3\n","delta = 0.03\n","s_min = 0.0\n","s_max = 20.0\n","t_min = 0.0\n","t_max = 1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699280289384,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"pxIV9pU2ZmwI"},"outputs":[],"source":["def initial(v):\n","  return np.maximum(K - v[:,1:2],0)\n","\n","def boundary_1(v):\n","    return K * np.exp(-1* r * (t_max - v[:,0:1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699280289384,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"KueZRwQSoh-F"},"outputs":[],"source":["def u(v):\n","    ans = []\n","    n_d1 = []\n","    n_d2 = []\n","    for i in range(len(v[:,0:1])):\n","        if v[i,0:1] == 1:\n","            ans = np.append(ans,max(K - v[i, 1:2], 0))\n","        elif v[i, 1:2] == 0:\n","            ans = np.append(ans, K * np.exp(-1* r * (t_max - v[i,0:1])))\n","        else:\n","            d1 = (math.log(v[i,1:2]/K) + (r - delta + 0.5 * sigma**2)*(t_max - v[i,0:1]))/(sigma * math.sqrt(t_max - v[i,0:1]))\n","            d2 = d1 - (sigma * math.sqrt(t_max - v[i,0:1]))\n","            n_d1 = stats.norm.cdf(-1 * d1)\n","            n_d2 = stats.norm.cdf(-1 * d2)\n","            temp = K* np.exp(-1* r * (t_max - v[i,0:1])) * n_d2 - v[i,1:2]* np.exp(-1*delta*(t_max-v[i,0:1]))* n_d1\n","            ans = np.append(ans, max(temp, max(0, K - v[i, 1:2])))\n","    return ans"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699280289384,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"PVXRlFYQowTb"},"outputs":[],"source":["def f(v):\n","    return v[:,1:2]*0\n","\n","def operator(v, t, s, r, sigma, delta, K):\n","    v_t = tf.gradients(v,t)[0]\n","    v_s = tf.gradients(v,s)[0]\n","    v_ss = tf.gradients(v_s,s)[0]\n","    pdp = v_t + 0.5* sigma**2 * s**2 * v_ss + (r - delta)*s*v_s - r*v\n","    d1 = (tf.math.log(s/K) + (r - delta + 0.5 * sigma**2)*(t_max - t))/(sigma * tf.math.sqrt(t_max - t))\n","    d2 = d1 - (sigma * tf.math.sqrt(t_max - t))\n","    n_d1 = tfp.distributions.Normal(0,1).cdf(-1 * d1)\n","    n_d2 = tfp.distributions.Normal(0,1).cdf(-1 * d2)\n","    ans = K* tf.exp(-1* r * (t_max - t)) * n_d2 - s* tf.exp(-1*delta*(t_max-t))* n_d1\n","    residual = (ans - tf.maximum(K - s,0))* pdp\n","    return residual"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":599206,"status":"ok","timestamp":1699281642291,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"qOJqI0ALNEEs","outputId":"d494b2a0-b61a-405b-aa98-128ab71d2f1f"},"outputs":[],"source":["# Domain Boundaries\n","ic_coords = np.array([[t_max,s_min],[t_max,s_max]])\n","bc1_coords = np.array([[t_min,s_min],[t_max, s_min]])\n","dom_coords = np.array([[t_min,s_min],[t_max,s_max]])\n","\n","# Create initial conditions random data\n","ic = Sampler(2, ic_coords, lambda v: initial(v), name = 'Initial Condition')\n","\n","# Create boundary conditions random data\n","bc = Sampler(2, bc1_coords, lambda v: boundary_1(v), name = 'Boundary 1 Condition')\n","\n","# Create residual conditions random data\n","res = Sampler(2, dom_coords, lambda v: f(v), name = 'Residual')\n","\n","# Define model\n","layers = [2,50,50,50,50,1]\n","model = 'M1'\n","stiff_ratio = False\n","model = Black_Scholes(layers, operator, ic, bc, res, r, sigma, delta, K, model, stiff_ratio)\n","\n","# Train model\n","model.train(nIter = 40000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2044,"status":"ok","timestamp":1699281744662,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"b2qMZKu4Nn7N","outputId":"15c6f7f5-f61e-4b39-e233-6adc9240821e"},"outputs":[],"source":["# Test data\n","nn = 100\n","t = np.linspace(dom_coords[0,0], dom_coords[1,0], nn)[:, None]\n","s = np.linspace(dom_coords[0,1], dom_coords[1,1], nn)[:, None]\n","t, s = np.meshgrid(t, s)\n","X_star = np.hstack((t.flatten()[:, None], s.flatten()[:, None]))\n","\n","# Predictions\n","v_pred = model.predict_v(X_star)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"elapsed":1861,"status":"ok","timestamp":1699281649130,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"wvo95E8zNqyV","outputId":"d4729905-f799-4f18-b940-090ea04ca236"},"outputs":[],"source":["# Plot\n","V_pred = griddata(X_star, v_pred.flatten(), (t, s), method='cubic')\n","\n","# untuk mencari nilai min dan max\n","V_min , V_max = np.amin(V_pred), np.amax(V_pred)\n","\n","fig_1 = plt.figure(1, figsize=(18, 5))\n","plt.subplot(1, 3, 1)\n","plt.pcolor(t, s, V_pred, cmap='jet', vmin = V_min, vmax = V_max)\n","plt.colorbar()\n","plt.xlabel('$t$')\n","plt.ylabel('$S$')\n","plt.title('Predicted V(S,t)')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize = (9,6))\n","ax = fig.add_subplot(111, projection = '3d')\n","ax.plot_surface(t, s, V_pred, cmap = \"viridis\")\n","ax.view_init(40,40)\n","ax.set_xlabel('$t$')\n","ax.set_ylabel('$S$')\n","ax.set_zlabel('$V(S,t)$')\n","ax.set_title('Predicted call option price')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699281654232,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"-aCZ3s8LN2l6","outputId":"beb4c8de-20e9-48c6-c5c8-50ee0f5dda36"},"outputs":[],"source":["# Loss\n","loss_r = model.loss_r_log\n","loss_ic = model.loss_ic_log\n","loss_bc = model.loss_bc_log\n","\n","\n","fig_2 = plt.figure(2)\n","ax = fig_2.add_subplot(1, 1, 1)\n","ax.plot(loss_r, label='$\\mathcal{L}_{r}$')\n","ax.plot(loss_ic, label='$\\mathcal{L}_{u_0}$')\n","ax.plot(loss_bc, label='$\\mathcal{L}_{u_b}$')\n","ax.plot()\n","ax.set_yscale('log')\n","ax.set_xlabel('iterations')\n","ax.set_ylabel('Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_loss_ic_1.npy',loss_ic)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_loss_bc_1.npy',loss_bc)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_loss_res_1.npy',loss_r)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699281654234,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"MG1r_hPBN43N","outputId":"7b59b77d-5c64-4161-a59c-35480afbb77a"},"outputs":[],"source":["# Adaptive Constant\n","adaptive_constant_ic = model.adaptive_constant_ic_log\n","adaptive_constant_bc = model.adaptive_constant_bc_log\n","\n","fig_3 = plt.figure(3)\n","ax = fig_3.add_subplot(1, 1, 1)\n","ax.plot(adaptive_constant_ic, label='$\\lambda_{u_0}$')\n","ax.plot(adaptive_constant_bc, label='$\\lambda_{u_b}$')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_adaptive_constant_ic_1.npy',adaptive_constant_ic)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_adaptive_constant_bc_1.npy',adaptive_constant_bc)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699281654235,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"2Vk6F7nViqDE"},"outputs":[],"source":["# Gradients at the end of training\n","data_gradients_ic = model.dict_gradients_ic_layers\n","data_gradients_bc = model.dict_gradients_bc_layers\n","data_gradients_res = model.dict_gradients_res_layers\n","\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_gradients_ic_1.npy',data_gradients_ic)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_gradients_bc_1.npy',data_gradients_bc)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/ap_gradients_res_1.npy',data_gradients_res)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2602,"status":"ok","timestamp":1699281657258,"user":{"displayName":"Derelith","userId":"07642643057046486959"},"user_tz":-420},"id":"1xni5BJ0iqDF","outputId":"900b80ae-2b47-4407-e83e-a18bc9883b8a"},"outputs":[],"source":["num_hidden_layers = len(layers) - 1\n","for j in range(num_hidden_layers):\n","  data = {'gradient_ic': data_gradients_ic['layer_' + str(j + 1)][-1],\n","          'gradient_bc': data_gradients_bc['layer_' + str(j + 1)][-1],\n","          'gradient_res': data_gradients_res['layer_' + str(j + 1)][-1]}\n","  data = pd.DataFrame(data)\n","\n","  sns.displot(data, kind = \"kde\", common_norm = False)\n","\n","  plt.title(\"Gradients Layer\" + str(j + 1))\n","  plt.tight_layout()\n","  plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
