{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3240,"status":"ok","timestamp":1699964143943,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"E05miAt_c2PF"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import timeit\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math\n","import pandas as pd\n","from scipy.interpolate import griddata\n","from scipy import stats"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21385,"status":"ok","timestamp":1699964165322,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"bOku5_rVHayB","outputId":"b9802d33-a87c-47d3-f58b-07a4de119200"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":65,"status":"ok","timestamp":1699964165323,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"9YoaYds8Y9wd"},"outputs":[],"source":["class Sampler:\n","    def __init__(self, dim, coords, func, name = None):\n","        self.dim = dim\n","        self.coords = coords\n","        self.func = func\n","        self.name = name\n","    def sample(self, N):\n","        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n","        y = self.func(x)\n","        return x, y\n","\n","class Black_Scholes:\n","\n","    # Disable eager execution\n","    tf.compat.v1.disable_eager_execution()\n","\n","    def __init__(self, layers, operator, ic, bc, res, r, sigma, delta, model, stiff_ratio):\n","\n","        self.operator = operator\n","        self.ic = ic\n","        self.bc = bc\n","        self.res = res\n","\n","        self.model = model\n","        self.stiff_ratio = stiff_ratio\n","\n","        # Constant\n","        self.sigma = tf.constant(sigma, dtype=tf.float32)\n","        self.r = tf.constant(r, dtype = tf.float32)\n","        self.delta = tf.constant(delta, dtype = tf.float32)\n","\n","        # Decay rate = 0.9\n","        self.rate = 0.9\n","        self.adaptive_constant_ic_val = np.array(1.0) # konstanta ic\n","        self.adaptive_constant_bc_val = np.array(1.0) # konstanta bc\n","\n","        # Initial weight + bias\n","        self.layers = layers\n","        self.weight, self.bias = self.initialize_NN(layers)\n","\n","        if model in ['M3', 'M4']:\n","            self.encoder_weight_1 = self.xavier_init([2, layers[1]])\n","            self.encoder_bias_1 = self.xavier_init([1,layers[1]])\n","\n","            self.encoder_weight_2 = self.xavier_init([2, layers[1]])\n","            self.encoder_bias_2 = self.xavier_init([1, layers[1]])\n","\n","        self.sess = tf.compat.v1.Session(config = tf.compat.v1.ConfigProto(log_device_placement = True))\n","\n","        # Define Placeholders and Computational Graph\n","        self.t_v_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_v_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.t_ic_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_ic_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.v_ic_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.t_bc1_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_bc1_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.v_bc1_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.t_r_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.s_r_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","        self.res_tf = tf.compat.v1.placeholder(tf.float32, shape = (None,1))\n","\n","        self.adaptive_constant_ic_tf = tf.compat.v1.placeholder(tf.float32, shape=self.adaptive_constant_ic_val.shape)\n","        self.adaptive_constant_bc_tf = tf.compat.v1.placeholder(tf.float32, shape=self.adaptive_constant_bc_val.shape)\n","\n","        # Evaluate Predictions\n","        self.v_ic_pred = self.net_u(self.t_ic_tf, self.s_ic_tf)\n","        self.v_bc1_pred = self.net_u(self.t_bc1_tf, self.s_bc1_tf)\n","\n","        self.v_pred = self.net_u(self.t_v_tf, self.s_v_tf)\n","        self.res_pred = self.net_r(self.t_r_tf, self.s_r_tf)\n","\n","        # Initial and Boundary Loss\n","        self.loss_ic = tf.reduce_mean(tf.square(self.v_ic_tf - self.v_ic_pred))\n","        self.loss_bc1 = tf.reduce_mean(tf.square(self.v_bc1_pred - self.v_bc1_tf))\n","\n","        self.loss_bcs = self.adaptive_constant_bc_tf * (self.loss_bc1)\n","        self.loss_ics = self.adaptive_constant_ic_tf * (self.loss_ic)\n","        self.loss_v = self.loss_ics + self.loss_bcs\n","\n","        self.loss_res = tf.reduce_mean(tf.square(self.res_pred))\n","        # - self.res_tf\n","        # Total Loss\n","        self.loss = self.loss_res + self.loss_v\n","\n","        # Define optimizer with learning rate schedule\n","        self.global_step = tf.Variable(0, trainable=False)\n","        initial_learning_rate = 1e-3\n","        self.learning_rate = tf.compat.v1.train.exponential_decay(initial_learning_rate, self.global_step,\n","                                                        1000, 0.9, staircase=False)\n","\n","        # Passing global_step to minimize() will increment it at each step\n","        self.train_op = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step = self.global_step)\n","\n","        # Logger\n","        self.loss_ic_log = []\n","        self.loss_bc_log = []\n","        self.loss_r_log = []\n","        self.saver = tf.compat.v1.train.Saver()\n","\n","        # Generate dictionary for gradients storage\n","        self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n","        self.dict_gradients_ic_layers = self.generate_grad_dict(self.layers)\n","        self.dict_gradients_bc_layers = self.generate_grad_dict(self.layers)\n","\n","        # Gradients Storage\n","        self.grad_res = []\n","        self.grad_ic = []\n","        self.grad_bc = []\n","\n","        for i in range(len(self.layers) - 1):\n","            self.grad_res.append(tf.gradients(self.loss_res, self.weight[i])[0])\n","            self.grad_bc.append(tf.gradients(self.loss_bcs, self.weight[i])[0])\n","            self.grad_ic.append(tf.gradients(self.loss_ics, self.weight[i])[0])\n","\n","        # Store Adaptive Constant\n","        self.adaptive_constant_ic_log = []\n","        self.adaptive_constant_bc_log = []\n","\n","        # Compute the adaptive constant\n","        self.adaptive_constant_ic_list = []\n","        self.adaptive_constant_bc_list = []\n","\n","        self.max_grad_res_list = []\n","        self.mean_grad_bc_list = []\n","        self.mean_grad_ic_list = []\n","\n","        for i in range(len(self.layers) - 1):\n","            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i])))\n","            self.mean_grad_bc_list.append(tf.reduce_mean(tf.abs(self.grad_bc[i])))\n","            self.mean_grad_ic_list.append(tf.reduce_mean(tf.abs(self.grad_ic[i])))\n","\n","        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n","        self.mean_grad_bc = tf.reduce_mean(tf.stack(self.mean_grad_bc_list))\n","        self.mean_grad_ic = tf.reduce_mean(tf.stack(self.mean_grad_ic_list))\n","\n","        self.adaptive_constant_bc = self.max_grad_res / self.mean_grad_bc\n","        self.adaptive_constant_ic = self.max_grad_res / self.mean_grad_ic\n","\n","        # Stiff Ratio\n","        if self.stiff_ratio:\n","            self.Hessian, self.Hessian_ic, self.Hessian_bc, self.Hessian_res = self.get_H_op()\n","            self.eigenvalue, _ = tf.linalg.eigh(self.Hessian)\n","            self.eigenvalue_ic, _ = tf.linalg.eigh(self.Hessian_ic)\n","            self.eigenvalue_bc, _ = tf.linalg.eigh(self.Hessian_bc)\n","            self.eigenvalue_res, _ = tf.linalg.eigh(self.Hessian_res)\n","\n","            self.eigenvalue_log = []\n","            self.eigenvalue_ic_log = []\n","            self.eigenvalue_bc_log = []\n","            self.eigenvalue_res_log = []\n","\n","        # Initialize tensorflow variables\n","        init = tf.compat.v1.global_variables_initializer()\n","        self.sess.run(init)\n","\n","    # Create dictionary to store gradients\n","    def generate_grad_dict(self, layers):\n","        num = len(layers) - 1\n","        grad_dict = {}\n","        for i in range(num):\n","            grad_dict['layer_{}'.format(i + 1)] = []\n","        return grad_dict\n","\n","    # Save gradients\n","    def save_gradients(self, tf_dict):\n","        num_layers = len(self.layers)\n","        for i in range(num_layers - 1):\n","            grad_ic_value, grad_bc_value, grad_res_value = self.sess.run([self.grad_ic[i], self.grad_bc[i], self.grad_res[i]], feed_dict = tf_dict)\n","\n","            self.dict_gradients_ic_layers['layer_' + str(i + 1)].append(grad_ic_value.flatten())\n","            self.dict_gradients_bc_layers['layer_' + str(i + 1)].append(grad_bc_value.flatten())\n","            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n","        return None\n","\n","    # Compute Hessian\n","    def flatten(self, vectors):\n","        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis = 0)\n","\n","    def get_Hv(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_Hv_ic(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss_ics, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_Hv_bc(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_Hv_res(self, v):\n","        loss_gradients = self.flatten(tf.gradients(self.loss_res, self.weight))\n","        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n","        Hv_op = self.flatten(tf.gradients(vprod, self.weight))\n","        return Hv_op\n","\n","    def get_H_op(self):\n","        self.P = self.flatten(self.weight).get_shape().as_list()[0]\n","        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype = 'float32')\n","        H_ic = tf.map_fn(self.get_Hv_ic, tf.eye(self.P, self.P), dtype = 'float32')\n","        H_bc = tf.map_fn(self.get_Hv_bc, tf.eye(self.P, self.P), dtype = 'float32')\n","        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P), dtype = 'float32')\n","        return H, H_ic, H_bc, H_res\n","\n","    # Xavier initialization\n","    def xavier_init(self, size):\n","        in_dim = size[0]\n","        out_dim = size[1]\n","        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n","        return tf.Variable(tf.compat.v1.random_normal([in_dim, out_dim], dtype = tf.float32) * xavier_stddev, dtype=tf.float32)\n","\n","    # Initialize network weights and biases using Xavier initialization\n","    def initialize_NN(self, layers):\n","        weight = []\n","        bias = []\n","        num_layers = len(layers)\n","        for l in range(0, num_layers - 1):\n","            W = self.xavier_init(size = [layers[l], layers[l+1]])\n","            b = tf.Variable(tf.zeros([1, layers[l+1]], dtype = tf.float32), dtype = tf.float32)\n","            weight.append(W)\n","            bias.append(b)\n","        return weight, bias\n","\n","    # Evaluate the forward pass\n","    def forward_pass(self, H):\n","        if self.model in ['M1', 'M2']:\n","            num_layers = len(self.layers)\n","            for l in range(0, num_layers - 2):\n","                W = self.weight[l]\n","                b = self.bias[l]\n","                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n","            W = self.weight[-1]\n","            b = self.bias[-1]\n","            H = tf.add(tf.matmul(H, W), b)\n","            return H\n","\n","        if self.model in ['M3', 'M4']:\n","            num_layers = len(self.layers)\n","            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weight_1), self.encoder_bias_1))\n","            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weight_2), self.encoder_bias_2))\n","\n","            for l in range(0, num_layers - 2):\n","                W = self.weight[l]\n","                b = self.bias[l]\n","                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n","                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n","\n","            W = self.weight[-1]\n","            b = self.bias[-1]\n","            H = tf.add(tf.matmul(H, W), b)\n","            return H\n","\n","    # Forward pass for u\n","    # This code is for initial and boundary condition\n","    def net_u(self, t, s):\n","        v = self.forward_pass(tf.concat([t, s], 1))\n","        return v\n","\n","    # Forward pass for residual\n","    def net_r(self, t, s):\n","        v = self.net_u(t, s)\n","        residual = self.operator(v, t, s, self.r, self.sigma, self.delta)\n","        return residual\n","\n","    def fetch_minibatch(self, sampler, N):\n","        X, Y = sampler.sample(N)\n","        return X, Y\n","\n","    # Trains the model by minimizing the MSE loss\n","    def train(self, nIter = 10000):\n","        start_time = timeit.default_timer()\n","\n","        # Fetch boundary mini-batches\n","        s_ic_batch, v_ic_batch = self.fetch_minibatch(self.ic, 250)\n","        s_bc1_batch, v_bc1_batch = self.fetch_minibatch(self.bc, 250)\n","\n","        # Fetch residual mini-batch\n","        s_res_batch, f_res_batch = self.fetch_minibatch(self.res, 62500)\n","\n","        # Define a dictionary for associating placeholder with data\n","        tf_dict = {self.t_ic_tf: s_ic_batch[:, 0:1], self.s_ic_tf: s_ic_batch[:, 1:2],\n","                    self.v_ic_tf: v_ic_batch,\n","                    self.t_bc1_tf: s_bc1_batch[:, 0:1], self.s_bc1_tf: s_bc1_batch[:, 1:2],\n","                    self.v_bc1_tf: v_bc1_batch,\n","                    self.t_r_tf: s_res_batch[:, 0:1], self.s_r_tf: s_res_batch[:, 1:2],\n","                    self.res_tf: f_res_batch,\n","                    self.adaptive_constant_ic_tf: self.adaptive_constant_ic_val,\n","                    self.adaptive_constant_bc_tf: self.adaptive_constant_ic_val}\n","\n","        for it in range(nIter):\n","\n","            # Run the tensorflow session to minimize losss\n","            self.sess.run(self.train_op, tf_dict)\n","\n","            # Print\n","            if it % 10 == 0:\n","                elapsed = timeit.default_timer() - start_time\n","                loss_value = self.sess.run(self.loss, tf_dict)\n","                loss_ic_value, loss_bc_value, loss_r_value = self.sess.run([self.loss_ics, self.loss_bcs, self.loss_res], tf_dict)\n","\n","                if self.model in ['M2', 'M4']:\n","                    # Compute the adaptive constant\n","                    adaptive_constant_ic_val, adaptive_constant_bc_val = self.sess.run([self.adaptive_constant_ic, self.adaptive_constant_bc], tf_dict)\n","\n","                    self.adaptive_constant_ic_val = adaptive_constant_ic_val * (1.0 - self.rate) + self.rate * self.adaptive_constant_ic_val\n","                    self.adaptive_constant_bc_val = adaptive_constant_bc_val * (1.0 - self.rate) + self.rate * self.adaptive_constant_bc_val\n","\n","                # Store loss and adaptive weights\n","                self.loss_ic_log.append(loss_ic_value)\n","                self.loss_bc_log.append(loss_bc_value)\n","                self.loss_r_log.append(loss_r_value)\n","\n","                self.adaptive_constant_ic_log.append(self.adaptive_constant_ic_val)\n","                self.adaptive_constant_bc_log.append(self.adaptive_constant_bc_val)\n","\n","                print('It: %d, Loss: %.3e, Loss_u0: %.3e, Loss_ub: %.3e, Loss_r: %.3e, Time: %.2f' %\n","                      (it, loss_value, loss_ic_value, loss_bc_value, loss_r_value, elapsed))\n","                print(\"constant_ic_val: {:.3f}, constant_bc_val: {:.3f}\".format(\n","                    self.adaptive_constant_ic_val,\n","                    self.adaptive_constant_bc_val))\n","                start_time = timeit.default_timer()\n","\n","            # Compute the eigenvalues of the Hessian of losses\n","            if self.stiff_ratio:\n","                if it % 1000 == 0:\n","                    print(\"Eigenvalues information stored...\")\n","                    eigenvalue, eigenvalue_ic, eigenvalue_bc, eigenvalue_res = self.sess.run([self.eigenvalue, self.eigenvalue_ic, self.eigenvalue_bc, self.eigenvalue_res], tf_dict)\n","\n","                    self.eigenvalue_log.append(eigenvalue)\n","                    self.eigenvalue_ic_log.append(eigenvalue_ic)\n","                    self.eigenvalue_bc_log.append(eigenvalue_bc)\n","                    self.eigenvalue_res_log.append(eigenvalue_res)\n","\n","            if it % 10000 == 0:\n","                self.save_gradients(tf_dict)\n","                print(\"Gradients information stored...\")\n","\n","    # Evaluates predictions at test points\n","    def predict_v(self, s_star):\n","            tf_dict = {self.t_v_tf: s_star[:, 0:1], self.s_v_tf: s_star[:, 1:2]}\n","            v_star = self.sess.run(self.v_pred, tf_dict)\n","            return v_star\n","\n","    def predict_r(self, s_star):\n","        tf_dict = {self.t_r_tf: s_star[:, 0:1], self.s_r_tf: s_star[:, 1:2]}\n","        r_star = self.sess.run(self.res_pred, tf_dict)\n","        return r_star\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":61,"status":"ok","timestamp":1699964165324,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"D21NSoJZzXFx"},"outputs":[],"source":["K = 10\n","r = 0.05\n","sigma = 0.3\n","delta = 0.03\n","s_min = 0.0\n","s_max = 20.0\n","t_min = 0.0\n","t_max = 1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":60,"status":"ok","timestamp":1699964165325,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"0ARrimgR0IBw"},"outputs":[],"source":["def initial(v):\n","  return np.maximum(v[:,1:2]-K,0)\n","\n","def boundary_1(v):\n","    return v[:,1:2]*0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56,"status":"ok","timestamp":1699964165326,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"NVzgqdHP0TiE"},"outputs":[],"source":["def u(v):\n","    ans = []\n","    n_d1 = []\n","    n_d2 = []\n","    for i in range(len(v[:,0:1])):\n","        if v[i,0:1] == 1:\n","            ans = np.append(ans,max(v[i, 1:2] - K, 0))\n","        elif v[i, 1:2] == 0:\n","            ans = np.append(ans,0)\n","        else:\n","            d1 = (math.log(v[i,1:2]/K) + (r - delta + 0.5 * sigma**2)*(t_max - v[i,0:1]))/(sigma * math.sqrt(t_max - v[i,0:1]))\n","            d2 = d1 - (sigma * math.sqrt(t_max - v[i,0:1]))\n","            n_d1 = stats.norm.cdf(d1)\n","            n_d2 = stats.norm.cdf(d2)\n","            ans = np.append(ans, v[i,1:2]* np.exp(-1*delta*(t_max-v[i,0:1]))* n_d1 - K* np.exp(-1* r * (t_max - v[i,0:1])) * n_d2)\n","    return ans"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":61,"status":"ok","timestamp":1699964165333,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"9dMIiq6R0XZb"},"outputs":[],"source":["def f(v):\n","    return v[:,1:2]*0\n","\n","def operator(v, t, s, r, sigma, delta):\n","    v_t = tf.gradients(v,t)[0]\n","    v_s = tf.gradients(v,s)[0]\n","    v_ss = tf.gradients(v_s,s)[0]\n","    residual = v_t + 0.5* sigma**2 * s**2 * v_ss + (r - delta)*s*v_s - r*v\n","    return residual"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":61,"status":"ok","timestamp":1699964165335,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"ReD8DynO9HpW"},"outputs":[],"source":["# Domain Boundaries\n","ic_coords = np.array([[t_max,s_min],[t_max,s_max]])\n","bc1_coords = np.array([[t_min,s_min],[t_max, s_min]])\n","dom_coords = np.array([[t_min,s_min],[t_max,s_max]])\n","\n","# Create initial conditions random data\n","ic = Sampler(2, ic_coords, lambda v: initial(v), name = 'Initial Condition')\n","\n","bc = Sampler(2, bc1_coords, lambda v: boundary_1(v), name = 'Boundary 1 Condition')\n","\n","# Create residual conditions random data\n","res = Sampler(2, dom_coords, lambda v: f(v), name = 'Residual')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9469,"status":"ok","timestamp":1699964174746,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"7EKZpDHK2feg","outputId":"22b47613-3b91-41ef-847e-b3b143d87103"},"outputs":[],"source":["# Define model\n","layers = [2,50,50,50,50,1]\n","model = 'M1'\n","stiff_ratio = False\n","model = Black_Scholes(layers, operator, ic, bc, res, r, sigma, delta, model, stiff_ratio)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1334862,"status":"ok","timestamp":1699965509600,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"cDLRhC7Y9Rhj","outputId":"1c4522cb-0baf-488d-c197-3bef9d93c2bb"},"outputs":[],"source":["# Train model\n","model.train(nIter = 40000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2740,"status":"ok","timestamp":1699965512316,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"myDC3Kmi2o9t","outputId":"b38bdc5c-bf9d-4613-f618-4763972b2b00"},"outputs":[],"source":["# Test data\n","nn = 100\n","t = np.linspace(dom_coords[0,0], dom_coords[1,0], nn)[:, None]\n","s = np.linspace(dom_coords[0,1], dom_coords[1,1], nn)[:, None]\n","t, s = np.meshgrid(t, s)\n","X_star = np.hstack((t.flatten()[:, None], s.flatten()[:, None]))\n","\n","# Exact Solution\n","v_star = u(X_star)\n","f_star = f(X_star)\n","v_star = v_star.reshape(len(X_star[:,0:1]),1)\n","\n","# Predictions\n","v_pred = model.predict_v(X_star)\n","\n","error_v = np.linalg.norm(v_star - v_pred, 2)\n","print('Relative L2 error_v: {:.2e}'.format(error_v))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1699965512316,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"GnnaM11qpm4B","outputId":"3641da93-c1a2-43c8-d241-a4e1a913b3e5"},"outputs":[],"source":["# Test data initial\n","nn = 100\n","t1 = 1\n","s1 = np.linspace(dom_coords[0,1], dom_coords[1,1], nn)[:, None]\n","t1, s1 = np.meshgrid(t1, s1)\n","X1_star = np.hstack((t1.flatten()[:, None], s1.flatten()[:, None]))\n","\n","# t = 1 , S = bervariasi\n","# Exact Solution\n","v1_star = u(X1_star)\n","f1_star = f(X1_star)\n","v1_star = v1_star.reshape(len(X1_star[:,0:1]),1)\n","\n","# Predictions\n","v1_pred = model.predict_v(X1_star)\n","\n","error_v = np.linalg.norm(v1_star - v1_pred, 2)\n","print('Relative L2 error_v: {:.2e}'.format(error_v))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1699965512317,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"ZqtCgXUIsD9Q","outputId":"f88805e9-8991-4a83-d876-09a3f4388615"},"outputs":[],"source":["# Test data boundary\n","nn = 100\n","t2 = np.linspace(dom_coords[0,0], dom_coords[1,0], nn)[:, None]\n","s2 = 0\n","t2, s2 = np.meshgrid(t2, s2)\n","X2_star = np.hstack((t2.flatten()[:, None], s2.flatten()[:, None]))\n","\n","# t = bervariasi , S = 0\n","# Exact Solution\n","v2_star = u(X2_star)\n","f2_star = f(X2_star)\n","v2_star = v2_star.reshape(len(X2_star[:,0:1]),1)\n","\n","# Predictions\n","v2_pred = model.predict_v(X2_star)\n","\n","error_v = np.linalg.norm(v2_star - v2_pred, 2)\n","print('Relative L2 error_v: {:.2e}'.format(error_v))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2979,"status":"ok","timestamp":1699965515284,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"IzFSi3v6p8tE","outputId":"279f1a32-294f-4180-91ad-2b3a426970e3"},"outputs":[],"source":["# Test data collocation\n","nn = 100\n","t3 = np.linspace(dom_coords[0,0], dom_coords[1,0], nn)[:, None]\n","s3 = np.linspace(dom_coords[0,1], dom_coords[1,1], nn)[:, None]\n","t3, s3 = np.meshgrid(t3[1:99], s3[1:99])\n","X3_star = np.hstack((t3.flatten()[:, None], s3.flatten()[:, None]))\n","\n","# Exact Solution\n","v3_star = u(X3_star)\n","f3_star = f(X3_star)\n","v3_star = v3_star.reshape(len(X3_star[:,0:1]),1)\n","\n","# Predictions\n","v3_pred = model.predict_v(X3_star)\n","\n","error_v = np.linalg.norm(v3_star - v3_pred, 2)\n","print('Relative L2 error_v: {:.2e}'.format(error_v))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"elapsed":1993,"status":"ok","timestamp":1699965517262,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"-ZSvbxIu2pOm","outputId":"2144c228-57c0-45c5-811c-16e65ec664fb"},"outputs":[],"source":["# Plot\n","V_star = griddata(X_star, v_star.flatten(), (t, s), method='cubic')\n","F_star = griddata(X_star, f_star.flatten(), (t, s), method='cubic')\n","\n","V_pred = griddata(X_star, v_pred.flatten(), (t, s), method='cubic')\n","\n","# untuk mencari nilai min dan max\n","V_comb = np.array([V_star,V_pred])\n","V_min , V_max = np.amin(V_comb), np.amax(V_comb)\n","\n","fig_1 = plt.figure(1, figsize=(18, 5))\n","plt.subplot(1, 3, 1)\n","plt.pcolor(t, s, V_star, cmap='jet', vmin = V_min, vmax = V_max)\n","plt.colorbar()\n","plt.xlabel('$t$')\n","plt.ylabel('$S$')\n","plt.title('Exact V(S,t)')\n","\n","plt.subplot(1, 3, 2)\n","plt.pcolor(t, s, V_pred, cmap='jet', vmin = V_min, vmax = V_max)\n","plt.colorbar()\n","plt.xlabel('$t$')\n","plt.ylabel('$S$')\n","plt.title('Predicted V(S,t)')\n","\n","plt.subplot(1, 3, 3)\n","plt.pcolor(t, s, np.abs(V_star - V_pred), cmap='jet', vmin = V_min, vmax = V_max)\n","plt.colorbar()\n","plt.xlabel('$t$')\n","plt.ylabel('$x$')\n","plt.title('Absolute error')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":396,"status":"ok","timestamp":1699965517645,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"AYzvZzxKgR8e","outputId":"c9847643-09ef-4e9a-b686-bec9d3aca560"},"outputs":[],"source":["plt.pcolor(t, s, np.abs(V_star - V_pred), cmap='jet')\n","plt.colorbar()\n","plt.xlabel('$t$')\n","plt.ylabel('$x$')\n","plt.title('Absolute error')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"executionInfo":{"elapsed":1856,"status":"ok","timestamp":1699965519498,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"m6NLugQm2xCg","outputId":"eafc028f-e5f2-4050-bc26-fb12499cb353"},"outputs":[],"source":["# 3D plot\n","V_star = griddata(X_star, v_star.flatten(), (t, s), method='cubic')\n","F_star = griddata(X_star, f_star.flatten(), (t, s), method='cubic')\n","V_pred = griddata(X_star, v_pred.flatten(), (t, s), method='cubic')\n","\n","# Surface plot of solution u(t,x)\n","fig = plt.figure(figsize = (9,6))\n","ax = fig.add_subplot(111, projection = '3d')\n","ax.plot_surface(t, s, V_star, cmap = \"viridis\")\n","ax.view_init(40,40)\n","ax.set_xlabel('$t$')\n","ax.set_ylabel('$S$')\n","ax.set_zlabel('$V(S,t)$')\n","ax.set_title('Exact call option price')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"executionInfo":{"elapsed":1138,"status":"ok","timestamp":1699965520630,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"CWJklJqx20XN","outputId":"235c5d04-dd79-4010-fd4e-6e011774de0b"},"outputs":[],"source":["fig = plt.figure(figsize = (9,6))\n","ax = fig.add_subplot(111, projection = '3d')\n","ax.plot_surface(t, s, V_pred, cmap = \"viridis\")\n","ax.view_init(40,40)\n","ax.set_xlabel('$t$')\n","ax.set_ylabel('$S$')\n","ax.set_zlabel('$V(S,t)$')\n","ax.set_title('Predicted call option price')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1699965520631,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"iRv_N2YE25LZ","outputId":"1727e648-8917-47e7-bdc1-ed946d3ce03f"},"outputs":[],"source":["# Loss\n","loss_r = model.loss_r_log\n","loss_ic = model.loss_ic_log\n","loss_bc = model.loss_bc_log\n","\n","fig_2 = plt.figure(2)\n","ax = fig_2.add_subplot(1, 1, 1)\n","ax.plot(loss_r, label='$\\mathcal{L}_{r}$')\n","ax.plot(loss_ic, label='$\\mathcal{L}_{u_0}$')\n","ax.plot(loss_bc, label='$\\mathcal{L}_{u_b}$')\n","ax.plot()\n","ax.set_yscale('log')\n","ax.set_xlabel('iterations')\n","ax.set_ylabel('Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","np.save('/content/gdrive/My Drive/Colab Notebooks/loss_ic_1.npy',loss_ic)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/loss_bc_1.npy',loss_bc)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/loss_res_1.npy',loss_r)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":817,"status":"ok","timestamp":1699965521439,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"MSGn2R3j27IH","outputId":"a6859caf-2541-41cd-f297-b6b0ad44f925"},"outputs":[],"source":["# Adaptive Constant\n","adaptive_constant_ic = model.adaptive_constant_ic_log\n","adaptive_constant_bc = model.adaptive_constant_bc_log\n","\n","fig_3 = plt.figure(3)\n","ax = fig_3.add_subplot(1, 1, 1)\n","ax.plot(adaptive_constant_ic, label='$\\lambda_{u_0}$')\n","ax.plot(adaptive_constant_bc, label='$\\lambda_{u_b}$')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","np.save('/content/gdrive/My Drive/Colab Notebooks/adaptive_constant_ic_1.npy',adaptive_constant_ic)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/adaptive_constant_bc_1.npy',adaptive_constant_bc)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1699965521440,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"Au_3zm61u9DM"},"outputs":[],"source":["# Gradients at the end of training\n","data_gradients_ic = model.dict_gradients_ic_layers\n","data_gradients_bc = model.dict_gradients_bc_layers\n","data_gradients_res = model.dict_gradients_res_layers\n","\n","np.save('/content/gdrive/My Drive/Colab Notebooks/gradients_ic_1.npy',data_gradients_ic)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/gradients_bc_1.npy',data_gradients_bc)\n","np.save('/content/gdrive/My Drive/Colab Notebooks/gradients_res_1.npy',data_gradients_res)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1699965521441,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"ygXlTuSO3V1B","outputId":"3b4f5c3d-7893-4b91-ed42-cc91aa919073"},"outputs":[],"source":["V_star = griddata(X_star, v_star.flatten(), (t, s), method='cubic')\n","F_star = griddata(X_star, f_star.flatten(), (t, s), method='cubic')\n","\n","fig_1 = plt.figure(1, figsize=(8, 8))\n","plt.plot(s[:,-1],V_star[:,-1],marker = 'o')\n","plt.xlabel('$s$')\n","plt.ylabel('$V$')\n","plt.grid()\n","plt.title('Exact V(S,t)')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2626,"status":"ok","timestamp":1699965524045,"user":{"displayName":"Tsukiasa","userId":"16208279298147463138"},"user_tz":-420},"id":"AGaol3atrueB","outputId":"e9b3f86a-5ab0-477b-a6b9-e8ebdfa98d66"},"outputs":[],"source":["num_hidden_layers = len(layers) - 1\n","for j in range(num_hidden_layers):\n","  data = {'gradient_ic': data_gradients_ic['layer_' + str(j + 1)][-1],\n","          'gradient_bc': data_gradients_bc['layer_' + str(j + 1)][-1],\n","          'gradient_res': data_gradients_res['layer_' + str(j + 1)][-1]}\n","  data = pd.DataFrame(data)\n","\n","  sns.displot(data, kind = \"kde\", common_norm = False)\n","\n","  plt.title(\"Gradients Layer\" + str(j + 1))\n","  plt.tight_layout()\n","  plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
